{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The document below contains all the files in the codebase. For your convenience, this is the structure of the codebase:\n",
      "```\n",
      "retromae_pretrain/\n",
      "├── __init__.py\n",
      "├── arguments.py\n",
      "├── data.py\n",
      "├── enhancedDecoder.py\n",
      "├── modeling.py\n",
      "├── run.py\n",
      "├── trainer.py\n",
      "└── utils.py\n",
      "```\n",
      "\n",
      "Below are the contents of all the files in the codebase, separated by '---'.\n",
      "# __init__.py\n",
      "```\n",
      "\n",
      "```\n",
      "\n",
      "---\n",
      "# arguments.py\n",
      "```\n",
      "import os\n",
      "from dataclasses import dataclass, field\n",
      "from typing import Optional\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class DataTrainingArguments:\n",
      "    train_data: Optional[str] = field(\n",
      "        default=None, metadata={\"help\": \"Path to pretrain data\"}\n",
      "    )\n",
      "    tokenizer_name: Optional[str] = field(\n",
      "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
      "    )\n",
      "    max_seq_length: Optional[int] = field(\n",
      "        default=512,\n",
      "        metadata={\n",
      "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
      "                    \"than this will be truncated. Default to the max input length of the model.\"\n",
      "        },\n",
      "    )\n",
      "    encoder_mlm_probability: float = field(default=0.3, metadata={\"help\": \"mask ratio for encoder\"})\n",
      "    decoder_mlm_probability: float = field(default=0.5, metadata={\"help\": \"mask ratio for decoder\"})\n",
      "\n",
      "    def __post_init__(self):\n",
      "        if not os.path.exists(self.train_data):\n",
      "            raise FileNotFoundError(f\"cannot find file: {self.train_data}, please set a true path\")\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class ModelArguments:\n",
      "    \"\"\"\n",
      "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
      "    \"\"\"\n",
      "    model_name_or_path: Optional[str] = field(\n",
      "        default='bert-base-uncased',\n",
      "        metadata={\n",
      "            \"help\": \"The model checkpoint for weights initialization.\"\n",
      "                    \"Don't set if you want to train a model from scratch.\"\n",
      "        },\n",
      "    )\n",
      "    config_name: Optional[str] = field(\n",
      "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
      "    )\n",
      "```\n",
      "\n",
      "---\n",
      "# data.py\n",
      "```\n",
      "import os\n",
      "import random\n",
      "from copy import deepcopy\n",
      "from dataclasses import dataclass\n",
      "\n",
      "import torch.utils.data.dataset\n",
      "from datasets import Dataset, load_dataset, concatenate_datasets\n",
      "from transformers import DataCollatorForWholeWordMask\n",
      "\n",
      "from .utils import tensorize_batch\n",
      "\n",
      "\n",
      "class DatasetForPretraining(torch.utils.data.Dataset):\n",
      "    def __init__(self, data_dir):\n",
      "        if os.path.isdir(data_dir):\n",
      "            datasets = []\n",
      "            for file in os.listdir(data_dir):\n",
      "                print(f\"Loading {file}\")\n",
      "                file = os.path.join(data_dir, file)\n",
      "                datasets.append(self.load_dataset(file))\n",
      "            self.dataset = concatenate_datasets(datasets)\n",
      "        else:\n",
      "            print(f\"Loading {data_dir}\")\n",
      "            self.dataset = self.load_dataset(data_dir)\n",
      "\n",
      "    def load_dataset(self, file):\n",
      "        if file.endswith('.jsonl') or file.endswith('.json'):\n",
      "            return load_dataset('json', data_files=file)['train']\n",
      "        elif os.path.isdir(file):\n",
      "            return Dataset.load_from_disk(file)\n",
      "        else:\n",
      "            raise NotImplementedError(f\"Not support this file format:{file}\")\n",
      "\n",
      "    def __getitem__(self, item):\n",
      "        return self.dataset[item]['text']\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class RetroMAECollator(DataCollatorForWholeWordMask):\n",
      "    max_seq_length: int = 512\n",
      "    encoder_mlm_probability: float = 0.15\n",
      "    decoder_mlm_probability: float = 0.15\n",
      "\n",
      "    def __call__(self, examples):\n",
      "        input_ids_batch = []\n",
      "        attention_mask_batch = []\n",
      "        encoder_mlm_mask_batch = []\n",
      "        decoder_labels_batch = []\n",
      "        decoder_matrix_attention_mask_batch = []\n",
      "\n",
      "        for e in examples:\n",
      "\n",
      "            e_trunc = self.tokenizer.encode(e, max_length=self.max_seq_length, truncation=True)\n",
      "            tokens = [self.tokenizer._convert_id_to_token(tid) for tid in e_trunc]\n",
      "\n",
      "            self.mlm_probability = self.encoder_mlm_probability\n",
      "            text_encoder_mlm_mask = self._whole_word_mask(tokens)\n",
      "\n",
      "            self.mlm_probability = self.decoder_mlm_probability\n",
      "            mask_set = []\n",
      "            for _ in range(min(len(tokens), 128)):\n",
      "                mask_set.append(self._whole_word_mask(tokens))\n",
      "\n",
      "            text_matrix_attention_mask = []\n",
      "            for i in range(len(tokens)):\n",
      "                idx = random.randint(0, min(len(tokens), 128) - 1)\n",
      "                text_decoder_mlm_mask = deepcopy(mask_set[idx])\n",
      "                text_decoder_mlm_mask[i] = 1\n",
      "                text_matrix_attention_mask.append(text_decoder_mlm_mask)\n",
      "\n",
      "            input_ids_batch.append(torch.tensor(e_trunc))\n",
      "            attention_mask_batch.append(torch.tensor([1] * len(e_trunc)))\n",
      "            e_trunc[0] = -100\n",
      "            e_trunc[-1] = -100\n",
      "            decoder_labels_batch.append(torch.tensor(e_trunc))\n",
      "\n",
      "            encoder_mlm_mask_batch.append(torch.tensor(text_encoder_mlm_mask))\n",
      "            decoder_matrix_attention_mask_batch.append(1 - torch.tensor(text_matrix_attention_mask))\n",
      "\n",
      "        input_ids_batch = tensorize_batch(input_ids_batch, self.tokenizer.pad_token_id)\n",
      "        attention_mask_batch = tensorize_batch(attention_mask_batch, 0)\n",
      "        origin_input_ids_batch = input_ids_batch.clone()\n",
      "        encoder_mlm_mask_batch = tensorize_batch(encoder_mlm_mask_batch, 0)\n",
      "        encoder_input_ids_batch, encoder_labels_batch = self.torch_mask_tokens(input_ids_batch, encoder_mlm_mask_batch)\n",
      "        decoder_labels_batch = tensorize_batch(decoder_labels_batch, -100)\n",
      "        matrix_attention_mask_batch = tensorize_batch(decoder_matrix_attention_mask_batch, 0)\n",
      "\n",
      "        batch = {\n",
      "            \"encoder_input_ids\": encoder_input_ids_batch,\n",
      "            \"encoder_attention_mask\": attention_mask_batch,\n",
      "            \"encoder_labels\": encoder_labels_batch,\n",
      "            \"decoder_input_ids\": origin_input_ids_batch,\n",
      "            \"decoder_attention_mask\": matrix_attention_mask_batch,  # [B,L,L]\n",
      "            \"decoder_labels\": decoder_labels_batch,\n",
      "        }\n",
      "\n",
      "        return batch\n",
      "```\n",
      "\n",
      "---\n",
      "# enhancedDecoder.py\n",
      "```\n",
      "'''\n",
      "The codes are modified based on huggingface transformers library.\n",
      "'''\n",
      "\n",
      "import math\n",
      "from typing import Optional, Tuple\n",
      "\n",
      "import torch\n",
      "import torch.utils.checkpoint\n",
      "from torch import nn\n",
      "from transformers.modeling_utils import (\n",
      "    apply_chunking_to_forward,\n",
      "    find_pruneable_heads_and_indices,\n",
      "    prune_linear_layer,\n",
      ")\n",
      "from transformers.models.bert.modeling_bert import BertIntermediate, BertOutput, BertSelfOutput\n",
      "from transformers.utils import (\n",
      "    logging,\n",
      ")\n",
      "\n",
      "logger = logging.get_logger(__name__)\n",
      "\n",
      "\n",
      "class BertSelfAttention(nn.Module):\n",
      "    def __init__(self, config, position_embedding_type=None):\n",
      "        super().__init__()\n",
      "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
      "            raise ValueError(\n",
      "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
      "                f\"heads ({config.num_attention_heads})\"\n",
      "            )\n",
      "\n",
      "        self.num_attention_heads = config.num_attention_heads\n",
      "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
      "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
      "\n",
      "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
      "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
      "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
      "\n",
      "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
      "        self.position_embedding_type = position_embedding_type or getattr(\n",
      "            config, \"position_embedding_type\", \"absolute\"\n",
      "        )\n",
      "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
      "            self.max_position_embeddings = config.max_position_embeddings\n",
      "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
      "\n",
      "        self.is_decoder = config.is_decoder\n",
      "\n",
      "    def transpose_for_scores(self, x):\n",
      "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
      "        x = x.view(new_x_shape)\n",
      "        return x.permute(0, 2, 1, 3)\n",
      "\n",
      "    def forward(\n",
      "            self,\n",
      "            query,\n",
      "            key,\n",
      "            value,\n",
      "            attention_mask: Optional[torch.FloatTensor] = None,\n",
      "            head_mask: Optional[torch.FloatTensor] = None,\n",
      "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
      "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
      "            past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
      "            output_attentions: Optional[bool] = False,\n",
      "    ) -> Tuple[torch.Tensor]:\n",
      "        mixed_query_layer = self.query(query)\n",
      "\n",
      "        # If this is instantiated as a cross-attention module, the keys\n",
      "        # and values come from an encoder; the attention mask needs to be\n",
      "        # such that the encoder's padding tokens are not attended to.\n",
      "        is_cross_attention = encoder_hidden_states is not None\n",
      "\n",
      "        if is_cross_attention and past_key_value is not None:\n",
      "            # reuse k,v, cross_attentions\n",
      "            key_layer = past_key_value[0]\n",
      "            value_layer = past_key_value[1]\n",
      "            attention_mask = encoder_attention_mask\n",
      "        elif is_cross_attention:\n",
      "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
      "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
      "            attention_mask = encoder_attention_mask\n",
      "        elif past_key_value is not None:\n",
      "            key_layer = self.transpose_for_scores(self.key(key))\n",
      "            value_layer = self.transpose_for_scores(self.value(value))\n",
      "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
      "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
      "        else:\n",
      "            key_layer = self.transpose_for_scores(self.key(key))\n",
      "            value_layer = self.transpose_for_scores(self.value(value))\n",
      "\n",
      "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
      "\n",
      "        if self.is_decoder:\n",
      "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
      "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
      "            # key/value_states (first \"if\" case)\n",
      "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
      "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
      "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
      "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
      "            past_key_value = (key_layer, value_layer)\n",
      "\n",
      "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
      "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "\n",
      "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
      "            seq_length = query.size()[1]\n",
      "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=query.device).view(-1, 1)\n",
      "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=query.device).view(1, -1)\n",
      "            distance = position_ids_l - position_ids_r\n",
      "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
      "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
      "\n",
      "            if self.position_embedding_type == \"relative_key\":\n",
      "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
      "                attention_scores = attention_scores + relative_position_scores\n",
      "            elif self.position_embedding_type == \"relative_key_query\":\n",
      "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
      "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
      "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
      "\n",
      "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
      "        if attention_mask is not None:\n",
      "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
      "            attention_scores = attention_scores + attention_mask\n",
      "\n",
      "        # Normalize the attention scores to probabilities.\n",
      "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
      "\n",
      "        # This is actually dropping out entire tokens to attend to, which might\n",
      "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
      "        attention_probs = self.dropout(attention_probs)\n",
      "\n",
      "        # Mask heads if we want to\n",
      "        if head_mask is not None:\n",
      "            attention_probs = attention_probs * head_mask\n",
      "\n",
      "        context_layer = torch.matmul(attention_probs, value_layer)\n",
      "\n",
      "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
      "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
      "        context_layer = context_layer.view(new_context_layer_shape)\n",
      "\n",
      "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
      "\n",
      "        if self.is_decoder:\n",
      "            outputs = outputs + (past_key_value,)\n",
      "        return outputs\n",
      "\n",
      "\n",
      "class BertAttention(nn.Module):\n",
      "    def __init__(self, config, position_embedding_type=None):\n",
      "        super().__init__()\n",
      "        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)\n",
      "        self.output = BertSelfOutput(config)\n",
      "        self.pruned_heads = set()\n",
      "\n",
      "    def prune_heads(self, heads):\n",
      "        if len(heads) == 0:\n",
      "            return\n",
      "        heads, index = find_pruneable_heads_and_indices(\n",
      "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
      "        )\n",
      "\n",
      "        # Prune linear layers\n",
      "        self.self.query = prune_linear_layer(self.self.query, index)\n",
      "        self.self.key = prune_linear_layer(self.self.key, index)\n",
      "        self.self.value = prune_linear_layer(self.self.value, index)\n",
      "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
      "\n",
      "        # Update hyper params and store pruned heads\n",
      "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
      "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
      "        self.pruned_heads = self.pruned_heads.union(heads)\n",
      "\n",
      "    def forward(\n",
      "            self,\n",
      "            query: torch.Tensor,\n",
      "            key: torch.Tensor,\n",
      "            value: torch.Tensor,\n",
      "            attention_mask: Optional[torch.FloatTensor] = None,\n",
      "            head_mask: Optional[torch.FloatTensor] = None,\n",
      "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
      "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
      "            past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
      "            output_attentions: Optional[bool] = False,\n",
      "    ) -> Tuple[torch.Tensor]:\n",
      "        self_outputs = self.self(\n",
      "            query, key, value,\n",
      "            attention_mask,\n",
      "            head_mask,\n",
      "            encoder_hidden_states,\n",
      "            encoder_attention_mask,\n",
      "            past_key_value,\n",
      "            output_attentions,\n",
      "        )\n",
      "        attention_output = self.output(self_outputs[0], query)\n",
      "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
      "        return outputs\n",
      "\n",
      "\n",
      "class BertLayerForDecoder(nn.Module):\n",
      "    def __init__(self, config):\n",
      "        super().__init__()\n",
      "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
      "        self.seq_len_dim = 1\n",
      "        self.attention = BertAttention(config)\n",
      "        self.is_decoder = config.is_decoder\n",
      "        self.add_cross_attention = config.add_cross_attention\n",
      "        if self.add_cross_attention:\n",
      "            if not self.is_decoder:\n",
      "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
      "            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\")\n",
      "        self.intermediate = BertIntermediate(config)\n",
      "        self.output = BertOutput(config)\n",
      "\n",
      "    def forward(\n",
      "            self,\n",
      "            query: torch.Tensor,\n",
      "            key: torch.Tensor,\n",
      "            value: torch.Tensor,\n",
      "            attention_mask: Optional[torch.FloatTensor] = None,\n",
      "            head_mask: Optional[torch.FloatTensor] = None,\n",
      "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
      "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
      "            past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
      "            output_attentions: Optional[bool] = False,\n",
      "    ) -> Tuple[torch.Tensor]:\n",
      "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
      "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
      "        self_attention_outputs = self.attention(\n",
      "            query, key, value,\n",
      "            attention_mask,\n",
      "            head_mask,\n",
      "            output_attentions=output_attentions,\n",
      "            past_key_value=self_attn_past_key_value,\n",
      "        )\n",
      "        attention_output = self_attention_outputs[0]\n",
      "\n",
      "        # if decoder, the last output is tuple of self-attn cache\n",
      "        if self.is_decoder:\n",
      "            outputs = self_attention_outputs[1:-1]\n",
      "            present_key_value = self_attention_outputs[-1]\n",
      "        else:\n",
      "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
      "\n",
      "        cross_attn_present_key_value = None\n",
      "        if self.is_decoder and encoder_hidden_states is not None:\n",
      "            if not hasattr(self, \"crossattention\"):\n",
      "                raise ValueError(\n",
      "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
      "                )\n",
      "\n",
      "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
      "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
      "            cross_attention_outputs = self.crossattention(\n",
      "                attention_output,\n",
      "                attention_mask,\n",
      "                head_mask,\n",
      "                encoder_hidden_states,\n",
      "                encoder_attention_mask,\n",
      "                cross_attn_past_key_value,\n",
      "                output_attentions,\n",
      "            )\n",
      "            attention_output = cross_attention_outputs[0]\n",
      "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
      "\n",
      "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
      "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
      "            present_key_value = present_key_value + cross_attn_present_key_value\n",
      "\n",
      "        layer_output = apply_chunking_to_forward(\n",
      "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
      "        )\n",
      "        outputs = (layer_output,) + outputs\n",
      "\n",
      "        # if decoder, return the attn key/values as the last output\n",
      "        if self.is_decoder:\n",
      "            outputs = outputs + (present_key_value,)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "    def feed_forward_chunk(self, attention_output):\n",
      "        intermediate_output = self.intermediate(attention_output)\n",
      "        layer_output = self.output(intermediate_output, attention_output)\n",
      "        return layer_output\n",
      "```\n",
      "\n",
      "---\n",
      "# modeling.py\n",
      "```\n",
      "import logging\n",
      "import os\n",
      "\n",
      "import torch\n",
      "from torch import nn\n",
      "from transformers import BertForMaskedLM, AutoModelForMaskedLM\n",
      "from transformers.modeling_outputs import MaskedLMOutput\n",
      "\n",
      "from .arguments import ModelArguments\n",
      "from .enhancedDecoder import BertLayerForDecoder\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class RetroMAEForPretraining(nn.Module):\n",
      "    def __init__(\n",
      "            self,\n",
      "            bert: BertForMaskedLM,\n",
      "            model_args: ModelArguments,\n",
      "    ):\n",
      "        super(RetroMAEForPretraining, self).__init__()\n",
      "        self.lm = bert\n",
      "\n",
      "        if hasattr(self.lm, 'bert'):\n",
      "            self.decoder_embeddings = self.lm.bert.embeddings\n",
      "        elif hasattr(self.lm, 'roberta'):\n",
      "            self.decoder_embeddings = self.lm.roberta.embeddings\n",
      "        else:\n",
      "            self.decoder_embeddings = self.lm.bert.embeddings\n",
      "\n",
      "        self.c_head = BertLayerForDecoder(bert.config)\n",
      "        self.c_head.apply(self.lm._init_weights)\n",
      "\n",
      "        self.cross_entropy = nn.CrossEntropyLoss()\n",
      "\n",
      "        self.model_args = model_args\n",
      "\n",
      "    def gradient_checkpointing_enable(self, **kwargs):\n",
      "        self.lm.gradient_checkpointing_enable(**kwargs)\n",
      "\n",
      "    def forward(self,\n",
      "                encoder_input_ids, encoder_attention_mask, encoder_labels,\n",
      "                decoder_input_ids, decoder_attention_mask, decoder_labels):\n",
      "\n",
      "        lm_out: MaskedLMOutput = self.lm(\n",
      "            encoder_input_ids, encoder_attention_mask,\n",
      "            labels=encoder_labels,\n",
      "            output_hidden_states=True,\n",
      "            return_dict=True\n",
      "        )\n",
      "        cls_hiddens = lm_out.hidden_states[-1][:, :1]  # B 1 D\n",
      "\n",
      "        decoder_embedding_output = self.decoder_embeddings(input_ids=decoder_input_ids)\n",
      "        hiddens = torch.cat([cls_hiddens, decoder_embedding_output[:, 1:]], dim=1)\n",
      "\n",
      "        # decoder_position_ids = self.lm.bert.embeddings.position_ids[:, :decoder_input_ids.size(1)]\n",
      "        # decoder_position_embeddings = self.lm.bert.embeddings.position_embeddings(decoder_position_ids)  # B L D\n",
      "        # query = decoder_position_embeddings + cls_hiddens\n",
      "\n",
      "        cls_hiddens = cls_hiddens.expand(hiddens.size(0), hiddens.size(1), hiddens.size(2))\n",
      "        query = self.decoder_embeddings(inputs_embeds=cls_hiddens)\n",
      "\n",
      "        matrix_attention_mask = self.lm.get_extended_attention_mask(\n",
      "            decoder_attention_mask,\n",
      "            decoder_attention_mask.shape,\n",
      "            decoder_attention_mask.device\n",
      "        )\n",
      "\n",
      "        hiddens = self.c_head(query=query,\n",
      "                              key=hiddens,\n",
      "                              value=hiddens,\n",
      "                              attention_mask=matrix_attention_mask)[0]\n",
      "        pred_scores, loss = self.mlm_loss(hiddens, decoder_labels)\n",
      "\n",
      "        return (loss + lm_out.loss,)\n",
      "\n",
      "    def mlm_loss(self, hiddens, labels):\n",
      "        if hasattr(self.lm, 'cls'):\n",
      "            pred_scores = self.lm.cls(hiddens)\n",
      "        elif hasattr(self.lm, 'lm_head'):\n",
      "            pred_scores = self.lm.lm_head(hiddens)\n",
      "        else:\n",
      "            raise NotImplementedError\n",
      "\n",
      "        masked_lm_loss = self.cross_entropy(\n",
      "            pred_scores.view(-1, self.lm.config.vocab_size),\n",
      "            labels.view(-1)\n",
      "        )\n",
      "        return pred_scores, masked_lm_loss\n",
      "\n",
      "    def save_pretrained(self, output_dir: str):\n",
      "        self.lm.save_pretrained(os.path.join(output_dir, \"encoder_model\"))\n",
      "        torch.save(self.state_dict(), os.path.join(output_dir, 'pytorch_model.bin'))\n",
      "\n",
      "    @classmethod\n",
      "    def from_pretrained(\n",
      "            cls, model_args: ModelArguments,\n",
      "            *args, **kwargs\n",
      "    ):\n",
      "        hf_model = AutoModelForMaskedLM.from_pretrained(*args, **kwargs)\n",
      "        model = cls(hf_model, model_args)\n",
      "        return model\n",
      "```\n",
      "\n",
      "---\n",
      "# run.py\n",
      "```\n",
      "import logging\n",
      "import os\n",
      "import sys\n",
      "\n",
      "import transformers\n",
      "from transformers import (\n",
      "    AutoTokenizer,\n",
      "    BertForMaskedLM,\n",
      "    AutoConfig,\n",
      "    HfArgumentParser, set_seed, )\n",
      "from transformers import (\n",
      "    TrainerCallback,\n",
      "    TrainingArguments,\n",
      "    TrainerState,\n",
      "    TrainerControl\n",
      ")\n",
      "from transformers.trainer_utils import is_main_process\n",
      "\n",
      "from .arguments import DataTrainingArguments, ModelArguments\n",
      "from .data import DatasetForPretraining, RetroMAECollator\n",
      "from .modeling import RetroMAEForPretraining\n",
      "from .trainer import PreTrainer\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class TrainerCallbackForSaving(TrainerCallback):\n",
      "    def on_epoch_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
      "        \"\"\"\n",
      "        Event called at the end of an epoch.\n",
      "        \"\"\"\n",
      "        control.should_save = True\n",
      "\n",
      "\n",
      "def main():\n",
      "    # See all possible arguments in src/transformers/training_args.py\n",
      "    # or by passing the --help flag to this script.\n",
      "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
      "\n",
      "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
      "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
      "        # If we pass only one argument to the script and it's the path to a json file,\n",
      "        # let's parse it to get our arguments.\n",
      "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
      "    else:\n",
      "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
      "\n",
      "    if (\n",
      "            os.path.exists(training_args.output_dir)\n",
      "            and os.listdir(training_args.output_dir)\n",
      "            and training_args.do_train\n",
      "            and not training_args.overwrite_output_dir\n",
      "    ):\n",
      "        raise ValueError(\n",
      "            f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"\n",
      "            \"Use --overwrite_output_dir to overcome.\"\n",
      "        )\n",
      "\n",
      "    model_args: ModelArguments\n",
      "    data_args: DataTrainingArguments\n",
      "    training_args: TrainingArguments\n",
      "\n",
      "    training_args.remove_unused_columns = False\n",
      "\n",
      "    # Setup logging\n",
      "    logging.basicConfig(\n",
      "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
      "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
      "        level=logging.INFO if is_main_process(training_args.local_rank) else logging.WARN,\n",
      "    )\n",
      "\n",
      "    # Log on each process the small summary:\n",
      "    logger.warning(\n",
      "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
      "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
      "    )\n",
      "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
      "    if is_main_process(training_args.local_rank):\n",
      "        transformers.utils.logging.set_verbosity_info()\n",
      "        transformers.utils.logging.enable_default_handler()\n",
      "        transformers.utils.logging.enable_explicit_format()\n",
      "    if training_args.local_rank in (0, -1):\n",
      "        logger.info(\"Training/evaluation parameters %s\", training_args)\n",
      "        logger.info(\"Model parameters %s\", model_args)\n",
      "        logger.info(\"Data parameters %s\", data_args)\n",
      "\n",
      "    set_seed(training_args.seed)\n",
      "\n",
      "    model_class = RetroMAEForPretraining\n",
      "    collator_class = RetroMAECollator\n",
      "\n",
      "    if model_args.model_name_or_path:\n",
      "        model = model_class.from_pretrained(model_args, model_args.model_name_or_path)\n",
      "        logger.info(f\"------Load model from {model_args.model_name_or_path}------\")\n",
      "        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n",
      "    elif model_args.config_name:\n",
      "        config = AutoConfig.from_pretrained(model_args.config_name)\n",
      "        bert = BertForMaskedLM(config)\n",
      "        model = model_class(bert, model_args)\n",
      "        logger.info(\"------Init the model------\")\n",
      "        tokenizer = AutoTokenizer.from_pretrained(data_args.tokenizer_name)\n",
      "    else:\n",
      "        raise ValueError(\"You must provide the model_name_or_path or config_name\")\n",
      "\n",
      "    dataset = DatasetForPretraining(data_args.train_data)\n",
      "\n",
      "    data_collator = collator_class(tokenizer,\n",
      "                                   encoder_mlm_probability=data_args.encoder_mlm_probability,\n",
      "                                   decoder_mlm_probability=data_args.decoder_mlm_probability,\n",
      "                                   max_seq_length=data_args.max_seq_length)\n",
      "\n",
      "    # Initialize our Trainer\n",
      "    trainer = PreTrainer(\n",
      "        model=model,\n",
      "        args=training_args,\n",
      "        train_dataset=dataset,\n",
      "        data_collator=data_collator,\n",
      "        tokenizer=tokenizer\n",
      "    )\n",
      "    trainer.add_callback(TrainerCallbackForSaving())\n",
      "\n",
      "    # # Training\n",
      "    trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
      "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "---\n",
      "# trainer.py\n",
      "```\n",
      "import logging\n",
      "import os\n",
      "from typing import Dict, Optional\n",
      "\n",
      "import torch\n",
      "from transformers import Trainer\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class PreTrainer(Trainer):\n",
      "    def log(self, logs: Dict[str, float]) -> None:\n",
      "        \"\"\"\n",
      "        Log `logs` on the various objects watching training.\n",
      "\n",
      "        Subclass and override this method to inject custom behavior.\n",
      "\n",
      "        Args:\n",
      "            logs (`Dict[str, float]`):\n",
      "                The values to log.\n",
      "        \"\"\"\n",
      "        logs[\"step\"] = self.state.global_step\n",
      "        if self.state.epoch is not None:\n",
      "            logs[\"epoch\"] = round(self.state.epoch, 2)\n",
      "\n",
      "        output = {**logs, **{\"step\": self.state.global_step}}\n",
      "        self.state.log_history.append(output)\n",
      "        self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n",
      "\n",
      "    def _save(self, output_dir: Optional[str] = None, state_dict=None):\n",
      "        output_dir = output_dir if output_dir is not None else self.args.output_dir\n",
      "        os.makedirs(output_dir, exist_ok=True)\n",
      "        logger.info(f\"Saving model checkpoint to {output_dir}\")\n",
      "        # Save a trained model and configuration using `save_pretrained()`.\n",
      "        # They can then be reloaded using `from_pretrained()`\n",
      "        if not hasattr(self.model, 'save_pretrained'):\n",
      "            logger.info(\"Trainer.model is not a `PreTrainedModel`, only saving its state dict.\")\n",
      "            state_dict = self.model.state_dict()\n",
      "            torch.save(state_dict, os.path.join(output_dir, \"pytorch_model.bin\"))\n",
      "        else:\n",
      "            self.model.save_pretrained(output_dir)\n",
      "        if self.tokenizer is not None:\n",
      "            self.tokenizer.save_pretrained(os.path.join(output_dir, \"encoder_model\"))\n",
      "\n",
      "        # Good practice: save your training arguments together with the trained model\n",
      "        torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n",
      "```\n",
      "\n",
      "---\n",
      "# utils.py\n",
      "```\n",
      "from typing import List\n",
      "\n",
      "import torch\n",
      "\n",
      "\n",
      "def tensorize_batch(sequences: List[torch.Tensor], padding_value, align_right=False) -> torch.Tensor:\n",
      "    if len(sequences[0].size()) == 1:\n",
      "        max_len_1 = max([s.size(0) for s in sequences])\n",
      "        out_dims = (len(sequences), max_len_1)\n",
      "        out_tensor = sequences[0].new_full(out_dims, padding_value)\n",
      "        for i, tensor in enumerate(sequences):\n",
      "            length_1 = tensor.size(0)\n",
      "            if align_right:\n",
      "                out_tensor[i, -length_1:] = tensor\n",
      "            else:\n",
      "                out_tensor[i, :length_1] = tensor\n",
      "        return out_tensor\n",
      "    elif len(sequences[0].size()) == 2:\n",
      "        max_len_1 = max([s.size(0) for s in sequences])\n",
      "        max_len_2 = max([s.size(1) for s in sequences])\n",
      "        out_dims = (len(sequences), max_len_1, max_len_2)\n",
      "        out_tensor = sequences[0].new_full(out_dims, padding_value)\n",
      "        for i, tensor in enumerate(sequences):\n",
      "            length_1 = tensor.size(0)\n",
      "            length_2 = tensor.size(1)\n",
      "            if align_right:\n",
      "                out_tensor[i, -length_1:, :length_2] = tensor\n",
      "            else:\n",
      "                out_tensor[i, :length_1, :length_2] = tensor\n",
      "        return out_tensor\n",
      "    else:\n",
      "        raise\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from repo_tree.repository_tree import FlatView\n",
    "path = 'https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding/retromae_pretrain'\n",
    "flat_view = FlatView.get_concatenated_file_contents(path)\n",
    "print(flat_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
